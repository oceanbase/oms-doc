# Deploy OMS on multiple nodes in multiple regions

This topic describes how to deploy OceanBase Migration Service (OMS) on multiple nodes in multiple regions by using deployment tools.

## Background

As more users apply OMS in data migration, OMS must adapt to increasingly diverse scenarios. In addition to single-region data migration and data synchronization, OMS supports data synchronization across regions, data migration between IDCs in different regions, and active-active data synchronization.

You can deploy OMS on one or more nodes in each region. OMS can be deployed on multiple nodes in a region to build an environment with high availability. In this way, OMS can start components on appropriate nodes based on the tasks.

For example, if you want to synchronize data from Region Hangzhou to Region Heilongjiang, OMS starts Store on a node in the Hangzhou region to collect the incremental logs and starts Incr-Sync on a node in the Heilongjiang region to synchronize the incremental data.

Notes:

* You can deploy OMS on a single node first and scale out to multiple nodes. For more information, see [Scale-out OMS](../400.deployment-guide/800.scale-out-oms.md).

* To deploy OMS on multiple nodes across multiple regions, you must apply for a virtual IP address (VIP) for each region and use it as the mount point for the OMS console. In addition, you must configure the mapping rules of Ports 8088 and 8089 in the VIP network strategy.

   You can use the VIP to access the OMS console even if an OMS node fails.

## Prerequisites

* The installation environment meets the system and network requirements. For more information, see [System and network requirements](../400.deployment-guide/200.system-requirements.md).

* A MetaDB cluster is in place to serve as the metadata database of OMS.

* Make sure that the server to deploy OMS can connect to all other servers.

* Make sure that all servers involved in the multi-node deployment can connect to each other and that you can obtain root permissions on a node by using its username and password.

* The OMS installation package is obtained. Generally, the package is a `tar.gz` file whose name starts with `oms`.

* The downloaded OMS installation package has been loaded into the local image repository of the Docker container on each server node.

   ```shell
   docker load -i <OMS installation package>
   ```

* The directory to which the OMS container is mounted has been prepared. OMS creates three mapping directories in this directory: `/home/admin/logs`, `/home/ds/store`, and `/home/ds/run`. These directories store the runtime components and logs of OMS.

* (Optional) A time-series database is installed to store OMS performance monitoring data and DDL and DML statistics.

## Concepts

You need to replace variable names in some commands and prompts. A variable name is enclosed with angle brackets (`<>`).

* Directory to which the OMS container is mounted: See the description of the mount directory in Prerequisites.

* IP address of the server: the IP address of the host that executes the script. In a single-node deployment scenario, by default, it refers to the IP address in the cluster management (CM) configuration information.

* OMS_IMAGE: the unique identifier of the loaded image. After you load the OMS installation package by using Docker, run the `docker images` command to obtain the [IMAGE ID] or [REPOSITORY:TAG] of the loaded image. The obtained value is the unique identifier of the loaded image. Example:

   ```shell
   $sudo docker images
   REPOSITORY                                          TAG                 IMAGE ID
   work.oceanbase-dev.com/obartifact-store/oms     feature_3.4.0       2a6a77257d35
   ```

   In this example, `<OMS_IMAGE>` can be `work.oceanbase-dev.com/obartifact-store/oms:feature_3.4.0` or `2a6a77257d35`. Replace the value of `<OMS_IMAGE>` in related commands with the preceding value.

* Directory of the `config.yaml` file: If you want to deploy OMS based on the current `config.yaml` configuration file, this directory is the one where the current configuration file is located.

## Deploy OMS without a configuration file

1. Log on to the server where OMS is deployed.

2. Optional. Deploy a time-series database.

   If you need to collect and display OMS monitoring data, deploy a time-series database. Otherwise, you can skip this step. For more information, see [Deploy a time-series database](../400.deployment-guide/1000.deploy-a-time-series-database.md).

3. Run the following command to obtain the deployment script from the loaded image:

   ```shell
   sudo docker run -d --name oms-config-tool <OMS_IMAGE> bash && sudo docker cp oms-config-tool:/root/docker_remote_deploy.sh . && sudo docker rm -f oms-config-tool
   ```

   Example:

   ```shell
   sudo docker run -d --net host --name oms-config-tool work.oceanbase-dev.com/obartifact-store/oms:feature_3.4.0 bash && sudo docker cp oms-config-tool:/root/docker_remote_deploy.sh . && sudo docker rm -f oms-config-tool
   ```

4. Use the deployment script to start the deployment tool.

   ```shell
   sh docker_remote_deploy.sh -o <deploy_tool_workdir> -i <IP address of the server> -d <OMS_IMAGE>
   ```

5. Follow the prompts to complete the deployment. After you set each parameter, press **Enter** to move on to the next parameter.

   1. Select the deployment mode.

      Select **Multiple Regions**.

   2. Select the task.

      Select **No Configuration File. Deploy OMS Starting from Configuration File Generation**.

   3. Enter the following MetaDB configuration information:

      1. Enter the URL, port, username, and password of the MetaDB.

      2. Set the prefix for names of databases in the MetaDB

         For example, when the prefix is set to `oms`, the databases in the MetaDB are named `oms_rm`, `oms_cm`, and `oms_cm_hb`.

      3. Confirm your settings.

         If the settings are correct, enter `y` and press **Enter** to proceed. Otherwise, enter `n` and press **Enter** to modify the settings.

      4. If the system displays "**The specified database names already exist in the MetaDB. Are you sure that you want to continue?**",  The message indicates that the database name you entered already exists in the MetaDB. This may be caused by repeated deployment or upgrade of OMS. You can enter `y` and press **Enter** to proceed, or enter `n` and press **Enter** to re-specify the settings.

   4. Perform the following operations to configure the OMS cluster settings:

      1. Enter the region ID, such as cn-hangzhou.

      2. Enter the region ID for the parameter cm_region_cn. It is the same as cm_region, for example, cn-hangzhou.

      3. Specify the URL of the CM service, which is the VIP or domain name to which all CM servers in the region are mounted. The original parameter is `cm-url`.

         Enter the VIP or domain name as the URL of the CM service. You can separately specify the IP address and port number in the URL, or use a colon (:) to join the IP address and port number in the <IP address>:<port number> format.

          <main id="notice" type='explain'>
          <h4>Note</h4>
          <p>The <code>http://</code> prefix in the URL is optional. </p>
          </main>

      4. Enter the IP addresses of all servers in the region. Separate them with commas (,).

      5. Set a region ID for the current region (Region name in Chinese). Value range: [0,127].

         An ID uniquely identifies a region.

      6. Specify whether to preferentially access the current region.

         In multi-region mode, set this parameter to `true` for only one region. If yes, enter `y` and press **Enter**. If not, enter `n` and press **Enter**.

      7. Confirm the displayed CM settings.

         If the settings are correct, enter `y` and press **Enter** to proceed. Otherwise, enter `n` and press **Enter** to modify the settings.

   5. Confirm whether to enable OMS historical data monitoring.

      * If you have deployed a time-series database, enter `y` and press **Enter** to to go to the next step to configure the time-series database and enable the monitoring of OMS historical data.

      * If you did not deploy a time-series database, enter `n` and press **Enter** to go to the step of "determining whether to enable the audit log feature and setting Simple Log Service (SLS) parameters". In this case, OMS does not monitor the historical data after deployment.

   6. Configure the time-series database.

      Perform the following operations:

      1. Confirm whether you have deployed a time-series database.

         Enter the value based on the actual situation. If yes, enter `y` and press **Enter**. If not, enter `n` and press **Enter** to go to the step of "determining whether to enable the audit log feature and setting SLS parameters".

      2. Set the type of the time-series database to `INFLUXDB`.

         <main id="notice" type='notice'>
         <h4>Notice</h4>
         <p>At present, only INFLUXDB is supported. </p>
         </main>

      3. Enter the URL, username, and password of the time-series database.

      4. Confirm whether the displayed settings are correct.

         If the settings are correct, enter `y` and press **Enter** to proceed. Otherwise, enter `n` and press **Enter** to modify the settings.

   7. Determine whether to enable the audit log feature and write the audit logs to Simple Log Service (SLS).

      To enable the audit log feature, enter `y` and press **Enter** to go to the next step to specify the SLS parameters.

      Otherwise, enter `n` and press **Enter** to start the deployment. In this case, OMS does not audit the logs after deployment.

   8. Specify the following SLS parameters:

      1. Set the SLS parameters as prompted.

      2. Confirm whether the displayed settings are correct.

      If the settings are correct, enter `y` and press **Enter** to proceed. Otherwise, enter `n` and press **Enter** to modify the settings.

   9. If the configuration file passes the check, all settings are displayed. If the settings are correct, enter `n` and press **Enter** to proceed. Otherwise, enter `y` and press **Enter** to modify the settings.

      If the configuration file fails the check, modify the configuration information as prompted.

   10. Start the deployment on each node one after another.

       1. Specify the directory to which the OMS container is mounted in the host.

          Use a directory with a large capacity.

          For a remote node, the username and password for logging on to the remote node are required. The corresponding user account must have the sudo privileges on the remote node.

       2. Confirm whether the OMS image file can be named as `OMS_IMAGE`.

          If the settings are correct, enter `y` and press **Enter** to proceed. Otherwise, enter `n` and press **Enter** to modify the settings.

       3. Confirm whether to install a Secure Sockets Layer (SSL) certificate for the OMS container.

          If yes, enter `y`, press **Enter**, and specify the `https_key` and `https_crt` directories as prompted. If not, enter `n` and press **Enter**.

   11. Confirm whether to deploy OMS in a new region.

       After the deployment is completed, the system displays "OMS has been deployed in Regions [<Region ID 1>,<Region ID 2>…]. Do you want to deploy OMS in a new region?"

       If yes, enter `y` and press **Enter** to proceed. If not, enter `n` and press **Enter** to end the deployment process.

   12. Perform the following operations to configure the OMS cluster settings:

       1. Enter the region ID, such as cn-hangzhou.

       2. Enter the region ID for the parameter cm_region_cn. It is the same as cm_region, for example, cn-hangzhou.

       3. Set a region ID for the current region (Region name in Chinese). Value range: [0,127].

          An ID uniquely identifies a region.

   13. A message is displayed, showing the names and IDs of existing regions, to help you avoid using an existing name or ID for a new region.

   14. Repeat the steps to deploy OMS on each node in the region.

       If the deployment fails, you can log on to the running OMS container and view the `.log` file with the `docker_init` prefix in the `/home/admin/logs` directory to obtain the log. If the OMS container fails to start, the logs cannot be obtained.

## Deploy OMS with a configuration file

1. Log on to the server where OMS is deployed.

2. Optional. Deploy a time-series database.

   If you need to collect and display OMS monitoring data, deploy a time-series database. Otherwise, you can skip this step. For more information, see [Deploy a time-series database](../400.deployment-guide/1000.deploy-a-time-series-database.md).

3. Run the following command to obtain the deployment script from the loaded image:

   ```shell
   sudo docker run -d --name oms-config-tool <OMS_IMAGE> bash && sudo docker cp oms-config-tool:/root/docker_remote_deploy.sh . && sudo docker rm -f oms-config-tool
   ```

4. Use the deployment script to start the deployment tool.

   ```shell
   sh docker_remote_deploy.sh -o <deploy_tool_workdir> -c <directory of the config.yaml file> -i <IP address of the server> -d <OMS_IMAGE>
   ```

   <main id="notice" type='explain'>
    <h4>Note</h4>
    <p>For more information about settings of the <code>config.yaml</code> file, see the "Template and example of a configuration file" section. </p>
   </main>

5. Follow the prompts to complete the deployment. After you set each parameter, press **Enter** to move on to the next parameter.

   1. Select the deployment mode.

      Select **Multiple Nodes in Single Region**.

   2. Select the task.

      Select **Use Configuration File Uploaded with Script Option [-c]**.

   3. If the system displays "**The specified database names already exist in the MetaDB. Are you sure that you want to continue?**",  The message indicates that the database name you entered already exists in the MetaDB specified in the original configuration file. This may be caused by repeated deployment or upgrade of OMS. You can enter `y` and press **Enter** to proceed, or enter `n` and press **Enter** to re-specify the settings.

   4. If the configuration file passes the check, all settings are displayed. If the settings are correct, enter `n` and press **Enter** to proceed. Otherwise, enter `y` and press **Enter** to modify the settings.

      If the configuration file fails the check, modify the configuration information as prompted.

   5. Start the deployment on each node one after another.

      1. Specify the directory to which the OMS container is mounted in the host.

         Use a directory with a large capacity.

         For a remote node, the username and password for logging on to the remote node are required. The corresponding user account must have the sudo privileges on the remote node.

      2. Confirm whether the OMS image file can be named as `OMS_IMAGE`.

         If the settings are correct, enter `y` and press **Enter** to proceed. Otherwise, enter `n` and press **Enter** to modify the settings.

      3. Confirm whether to install a Secure Sockets Layer (SSL) certificate for the OMS container.

         If yes, enter `y`, press **Enter**, and specify the `https_key` and `https_crt` directories as prompted. If not, enter `n` and press **Enter**.

   6. Confirm whether to deploy OMS in a new region.

      After the deployment is completed, the system displays "OMS has been deployed in Regions [<Region ID 1>,<Region ID 2>…]. Do you want to deploy OMS in a new region?"

      If yes, enter `y` and press **Enter** to proceed. If not, enter `n` and press **Enter** to end the deployment process.

   7. Perform the following operations to configure the OMS cluster settings:

      1. Enter the region ID, such as cn-hangzhou.

      2. Enter the region ID for the parameter cm_region_cn. It is the same as cm_region, for example, cn-hangzhou.

      3. Set a region ID for the current region (Region name in Chinese). Value range: [0,127].

         An ID uniquely identifies a region.

   8. A message is displayed, showing the names and IDs of existing regions, to help you avoid using an existing name or ID for a new region.

   9. Repeat the steps to deploy OMS on each node in the region.

      If the deployment fails, you can log on to the running OMS container and view the `.log` file with the `docker_init` prefix in the `/home/admin/logs` directory to obtain the log. If the OMS container fails to start, the logs cannot be obtained.

To modify the configuration after deployment, log on to the OMS container and perform the following steps:

<main id="notice" type='notice'>
<h4>Notice</h4>
<p>If you deploy OMS on multiple nodes in multiple regions, you must manually modify the configuration of each node. </p>
</main>

1. Modify the `config.yaml` file based on business needs.

2. Run the `python -m omsflow.scripts.units.oms_init_manager --init-config-file` command.

3. Run the `supervisorctl restart oms_console oms_drc_supervisor` command.

## Template and example of a configuration file

### Configuration file template

  <main id="notice" type='notice'>
    <h4>Notice</h4>
    <ul>
    <li>
    <p>When multiple regions exist, you must set the <code>cm_is_default</code> parameter to <code>true</code> for only one region, and set it to <code>false</code> for all other regions. In addition, you must sequentially run commands in each region. </p>
    </li>
    <li>
    <p>To deploy multiple nodes in the Hangzhou region, specify the IP addresses of all nodes for the <code>cm_nodes</code> parameter. </p>
    </li>
    <li>
    <p>You must replace the sample values of required parameters based on your actual deployment environment. Both the required and optional parameters are described in the following table. You can specify the optional parameters as needed. </p>
    </li>
    <li>
    <p>In the <code>config.yaml</code> file, you must specify the parameters in the key: value format, with a space after the colon (:). </p>
    </li>
    </ul>
  </main>

In the following examples of the `config.yaml` file for the multi-node multi-region deployment mode, OMS is deployed on two nodes in the Hangzhou and Heilongjiang regions.

* The following example describes a template of the `config.yaml` file for you to deploy OMS in the Hangzhou region:

   ```yaml
   # Information about the OMS MetaDB
   oms_meta_host: ${oms_meta_host}
   oms_meta_port: ${oms_meta_port}
   oms_meta_user: ${oms_meta_user}
   oms_meta_password: ${oms_meta_password}
   
   # You can customize the names of the following three databases, which are created in the MetaDB when you deploy OMS.
   drc_rm_db: ${drc_rm_db}
   drc_cm_db: ${drc_cm_db}
   drc_cm_heartbeat_db: ${drc_cm_heartbeat_db}
   
   # Configure the OMS cluster in the Hangzhou region.
   # To deploy OMS on multiple nodes in multiple regions, you must set the cm_url parameter to a VIP or domain name to which all CM servers in the region are mounted.
   cm_url: ${cm_url}
   cm_location: ${cm_location}
   cm_region: ${cm_region}
   cm_region_cn: ${cm_region_cn}
   cm_is_default: true
   cm_nodes:
    - ${host_ip1}
    -  ${host_ip2}
   
   # Configurations of the time-series database
   # Default value: false. To enable metric reporting, set the parameter to `true`.
   # tsdb_enabled: false 
   # If the `tsdb_enabled` parameter is set to `true`, delete comments for the following parameters and specify the values based on your actual configurations.
   # tsdb_service: 'INFLUXDB'
   # tsdb_url: '${tsdb_url}'
   # tsdb_username: ${tsdb_user}
   # tsdb_password: ${tsdb_password}
   ```

   | Parameter | Description | Required?                     |
   |---------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------|
   | oms_meta_host | The IP address of the MetaDB, which can be the IP address of a MySQL database or a MySQL tenant of OceanBase Database.<br>  **Notice:**<br>This parameter is valid only in OceanBase Database V2.0 and later.  | Yes                           |
   | oms_meta_port | The port number of the MetaDB.  | Yes                           |
   | oms_meta_user | The username of the MetaDB.  | Yes                           |
   | oms_meta_password | The user password of the MetaDB.  | Yes                           |
   | drc_rm_db | The name of the database for the OMS console.  | Yes                           |
   | drc_cm_db | The name of the database for the CM service.  | Yes                           |
   | drc_cm_heartbeat_db | The name of the heartbeat database for the CM service.  | Yes                           |
   | cm_url | The URL of the OMS CM service. Example: <http://VIP:8088>.  <br>**Note:**<br>To deploy OMS on multiple nodes in multiple regions, you must set the `cm_url` parameter to a VIP or domain name to which all CM servers in the region are mounted. We do not recommend that you set it to `http://127.0.0.1:8088`.  <br>The access URL of the OMS console is in the format of `IP address of the host on which OMS is deployed:8089`. Example: `http(https)://xxx.xxx.xxx.xxx:8089`.  Port 8088 is used for program calls, and Port 8089 is used for web page access. You must specify Port 8088.  | Yes                           |
   | cm_location | The code of the region. Value range: [0,127]. You can select one number for each region.  <br>**Notice:**<br>If you upgrade to OMS V3.2.1 from an earlier version, you must set the `cm_location` parameter to `0`.  | Yes                           |
   | cm_region | The name of the region. Example: cn-hangzhou.  <br>**Notice:**<br>If you use OMS with the Alibaba Cloud Multi-Site High Availability (MSHA) service in an active-active disaster recovery scenario, use the region configured for the Alibaba Cloud service.  | Yes                           |
   | cm_region_cn | The value here is the same as the value of cm_region. | Yes                           |
   | cm_nodes | The IP addresses of servers on which the OMS CM service is deployed. In multi-node deployment mode, you must specify multiple IP addresses for the parameter.  | Yes                           |
   | cm_is_default | Indicates whether the OMS CM service is enabled by default.  | No. Default value: `true`.    |
   | tsdb_service | The type of the time-series database. Valid values: `INFLUXDB` and `CERESDB`.  | No. Default value: `CERESDB`. |
   | tsdb_enabled | Indicates whether metric reporting is enabled for monitoring. Valid values: `true` and `false`.  | No. Default value: `false`.   |
   | tsdb_url | The IP address of the server where InfluxDB is deployed, which needs to be modified based on the actual environment. You need to modify this parameter based on the actual environment if you set the `tsdb_enabled` parameter to `true`.  After the time-series database is deployed, it maps to OMS deployed for the whole cluster. This means that although OMS is deployed in multiple regions, all regions map to the same time-series database.  | No                            |
   | tsdb_username | The username used to connect to the time-series database. You need to modify this parameter based on the actual environment if you set the `tsdb_enabled` parameter to `true`.  After you deploy the time-series database, manually create a user and specify the username and password.  | No                            |
   | tsdb_password | The password used to connect to the time-series database. You need to modify this parameter based on the actual environment if you set the `tsdb_enabled` parameter to `true`.  | No                            |

* The following example describes a template of the `config.yaml` file for you to deploy OMS in the Heilongjiang region:

   The operations are the same as those for deploying OMS in the Hangzhou region, except that you must modify the following parameters in the `config.yaml` file: `drc_cm_heartbeat_db`, `cm_url`, `cm_location`, `cm_region`, `cm_is_default`, and `cm_nodes`.

   <main id="notice" type='notice'>
    <h4>Notice</h4>
    <ul>
    <li>
    <p>When multiple regions exist, you must set the <code>cm_is_default</code> parameter to <code>true</code> for only one region, and set it to <code>false</code> for all other regions. </p>
    </li>
    <li>
    <p>To deploy multiple nodes in the Heilongjiang region, specify the IP addresses of all nodes for the <code>cm_nodes</code> parameter. </p>
    </li>
    <li>
    <p>You must execute the <code>docker_init.sh</code> script on at least one node in each region. </p>
    </li>
    </ul>
   </main>

   ```yaml
   # Information about the OMS MetaDB
   oms_meta_host: ${meta_ip}
   oms_meta_port: ${meta_port}
   oms_meta_user: ${meta_user}
   oms_meta_password: ${meta_password}
   
   # You can customize the names of the following three databases, which are created in the MetaDB when you deploy OMS.
   drc_rm_db: ${drc_rm_db}
   drc_cm_db: ${drc_cm_db}
   drc_cm_heartbeat_db: ${drc_cm_heartbeat_db}
   
   # Configure the OMS cluster in the Heilongjiang region.
   # To deploy OMS on multiple nodes in multiple regions, you must set the cm_url parameter to a VIP or domain name to which all CM servers in the region are mounted.
   cm_url: ${cm_url}
   cm_location: ${cm_location}
   cm_region: ${cm_region}
   cm_region_cn: ${cm_region_cn}
   cm_is_default: false
   cm_nodes:
    - ${host_ip1}
    - ${host_ip2}
   
   # Configurations of the time-series database
   # tsdb_service: 'INFLUXDB'
   # Default value: false. Set the value based on your actual configuration.
   # tsdb_enabled: false
   
   # The IP address of the server where InfluxDB is deployed.
   # You need to modify the following parameters based on the actual environment if you set the tsdb_enabled parameter to true.
   # tsdb_url: ${tsdb_url}
   # tsdb_username: ${tsdb_user}
   # tsdb_password: ${tsdb_password}
   ```

### Example

Replace related parameters with the actual values in the target deployment environment.

* The following example of the `config.yaml` file shows sample settings when OMS is deployed in the Hangzhou region:

   ```yaml
   oms_meta_host: xxx.xxx.xxx.xxx
   oms_meta_port: 2883
   oms_meta_user: oms_meta_user
   oms_meta_password: *********
   drc_rm_db: oms_rm
   drc_cm_db: oms_cm
   drc_cm_heartbeat_db: oms_cm_heartbeat
   cm_url: http://VIP:8088
   cm_location: 1
   cm_region: cn-hangzhou
   cm_region_cn: cn-hangzhou
   cm_is_default: true
   cm_nodes:
    - xxx.xxx.xxx.xx1
    - xxx.xxx.xxx.xx2
   tsdb_service: 'INFLUXDB'
   tsdb_enabled: true
   tsdb_url: 'xxx.xxx.xxx.xxx:8086'
   tsdb_username: username
   tsdb_password: *************
   ```

* The following example of the `config.yaml` file shows sample settings when OMS is deployed in the Heilongjiang region:

   ```yaml
   oms_meta_host: xxx.xxx.xxx.xxx
   oms_meta_port: 2883
   oms_meta_user: oms_meta_user
   oms_meta_password: ***********
   drc_rm_db: oms_rm
   drc_cm_db: oms_cm
   drc_cm_heartbeat_db: oms_cm_heartbeat_1
   cm_url: http://VIP:8088
   cm_location: 2
   cm_region: cn-heilongjiang
   cm_region_cn: cn-heilongjiang
   cm_is_default: false
   cm_nodes:
    - xxx.xxx.xxx.xx1
    - xxx.xxx.xxx.xx2
   tsdb_service: 'INFLUXDB'
   tsdb_enabled: true
   tsdb_url: 'xxx.xxx.xxx.xxx:8086'
   tsdb_username: username
   tsdb_password: *************
   ```
